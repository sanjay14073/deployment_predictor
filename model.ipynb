{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24397a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas numpy scikit-learn matplotlib seaborn statsmodels prophet tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8317c795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d2c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV (do not use index_col so all columns are available)\n",
    "df = pd.read_csv('data/data.csv')\n",
    "print('Rows:', len(df))\n",
    "df.head()\n",
    "\n",
    "# Prepare hourly time series DataFrame\n",
    "df_hourly = df.groupby(['date', 'hour'])[['number_of_transactions', 'total_amount']].sum().reset_index()\n",
    "\n",
    "# construct a datetime 'ds' column\n",
    "df_hourly['ds'] = pd.to_datetime(df_hourly['date'].astype(str) + ' ' + df_hourly['hour'].astype(str) + ':00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9413a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Normalize the two impact columns (Min-Max Scaling)\n",
    "min_tx = df_hourly['number_of_transactions'].min()\n",
    "max_tx = df_hourly['number_of_transactions'].max()\n",
    "# Handle case where min == max (to avoid division by zero)\n",
    "range_tx = max_tx - min_tx\n",
    "df_hourly['norm_transactions'] = (df_hourly['number_of_transactions'] - min_tx) / range_tx if range_tx != 0 else 0\n",
    "\n",
    "min_amt = df_hourly['total_amount'].min()\n",
    "max_amt = df_hourly['total_amount'].max()\n",
    "range_amt = max_amt - min_amt\n",
    "df_hourly['norm_amount'] = (df_hourly['total_amount'] - min_amt) / range_amt if range_amt != 0 else 0\n",
    "\n",
    "\n",
    "# 2. Apply the 60:40 weighting to the normalized values to create the target variable 'y'\n",
    "# 60% weight on normalized transactions, 40% on normalized amount.\n",
    "WEIGHT_TX = 0.60\n",
    "WEIGHT_AMT = 0.40\n",
    "df_hourly['y'] = (df_hourly['norm_transactions'] * WEIGHT_TX) + (df_hourly['norm_amount'] * WEIGHT_AMT)\n",
    "\n",
    "# Select the necessary columns for Prophet\n",
    "df_ts = df_hourly[['ds', 'y']].sort_values('ds').reset_index(drop=True)\n",
    "print(\"\\nTarget Variable 'y' Head (Weighted Impact Score):\")\n",
    "print(df_ts.head())\n",
    "\n",
    "# Feature engineering (exogenous regressors)\n",
    "df_features = df_ts.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c65e28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A. Cyclical and Binary Features\n",
    "df_features['hour'] = df_features['ds'].dt.hour\n",
    "df_features['day_of_week'] = df_features['ds'].dt.dayofweek # Mon=0, Sun=6\n",
    "df_features['day_of_month'] = df_features['ds'].dt.day # Regressor for monthly cycles\n",
    "\n",
    "# Cyclical Encoding for Hour (24-hour cycle)\n",
    "df_features['hour_sin'] = np.sin(2 * np.pi * df_features['hour'] / 24)\n",
    "df_features['hour_cos'] = np.cos(2 * np.pi * df_features['hour'] / 24)\n",
    "\n",
    "# Cyclical Encoding for Day of Week (7-day cycle)\n",
    "df_features['day_sin'] = np.sin(2 * np.pi * df_features['day_of_week'] / 7)\n",
    "df_features['day_cos'] = np.cos(2 * np.pi * df_features['day_of_week'] / 7)\n",
    "\n",
    "# Is it a Weekend?\n",
    "df_features['is_weekend'] = df_features['day_of_week'].isin([5,6]).astype(int)\n",
    "\n",
    "# Drop temporary raw columns\n",
    "df_features = df_features.drop(columns=['hour','day_of_week'])\n",
    "\n",
    "# Add lag features for daily and weekly seasonality\n",
    "df_features['lag_24h'] = df_features['y'].shift(24)\n",
    "df_features['lag_168h'] = df_features['y'].shift(168)\n",
    "\n",
    "# Drop initial rows with NaNs from lags\n",
    "df_prepared = df_features.dropna().reset_index(drop=True)\n",
    "print(f'\\nOriginal rows: {len(df_ts)}, After lag & dropna: {len(df_prepared)}')\n",
    "df_prepared.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f216694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / test split: use 7 days (one week) for test since the dataset is one month long\n",
    "TEST_SIZE = 7 * 24\n",
    "if TEST_SIZE >= len(df_prepared):\n",
    "    TEST_SIZE = max(24, len(df_prepared) // 5)  # fallback: 20% or at least 24 hours\n",
    "df_train = df_prepared.iloc[:-TEST_SIZE].reset_index(drop=True)\n",
    "df_test = df_prepared.iloc[-TEST_SIZE:].reset_index(drop=True)\n",
    "print(f'\\nTraining set size: {len(df_train)} rows.')\n",
    "print(f'Testing set size: {len(df_test)} rows ({TEST_SIZE} hours).')\n",
    "\n",
    "# Fit Prophet with all exogenous regressors, including the new 'day_of_month'\n",
    "model = Prophet(daily_seasonality=True, weekly_seasonality=True, yearly_seasonality=False, interval_width=0.95)\n",
    "exogenous_features = [\n",
    "    'hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'is_weekend',\n",
    "    'lag_24h', 'lag_168h', 'day_of_month' # <<< New feature added\n",
    "]\n",
    "\n",
    "for feature in exogenous_features:\n",
    "    model.add_regressor(feature)\n",
    "\n",
    "# Prophet expects columns: ds, y, and the added regressor columns in the training DataFrame\n",
    "model.fit(df_train[['ds','y'] + exogenous_features])\n",
    "\n",
    "# --- Evaluation (using the test set) ---\n",
    "df_future_test = df_test[['ds'] + exogenous_features].copy()\n",
    "forecast_test = model.predict(df_future_test)\n",
    "y_pred = forecast_test['yhat'].values\n",
    "y_true = df_test['y'].values\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "print('\\nModel Evaluation on Test Data:')\n",
    "print(f'Mean Absolute Error (MAE): {mae:,.4f}') # Increased precision since 'y' is now normalized\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse:,.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c787927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate Future Forecast for Next Month (30 days) ---\n",
    "FUTURE_DAYS = 30\n",
    "future = model.make_future_dataframe(periods=FUTURE_DAYS * 24, freq='H')\n",
    "\n",
    "# 1. Populate the exogenous features for the future period\n",
    "future['hour'] = future['ds'].dt.hour\n",
    "future['day_of_week'] = future['ds'].dt.dayofweek\n",
    "future['day_of_month'] = future['ds'].dt.day # New Regressor for monthly cycles\n",
    "\n",
    "future['hour_sin'] = np.sin(2 * np.pi * future['hour'] / 24)\n",
    "future['hour_cos'] = np.cos(2 * np.pi * future['hour'] / 24)\n",
    "future['day_sin'] = np.sin(2 * np.pi * future['day_of_week'] / 7)\n",
    "future['day_cos'] = np.cos(2 * np.pi * future['day_of_week'] / 7)\n",
    "future['is_weekend'] = future['day_of_week'].isin([5,6]).astype(int)\n",
    "\n",
    "# 2. Populate the Lag Features for the future period\n",
    "# Propagating historical lags and filling the recursive steps with the mean.\n",
    "mean_y = df_prepared['y'].mean()\n",
    "future['lag_24h'] = future['ds'].apply(lambda x: df_prepared[df_prepared['ds'] == (x - pd.Timedelta(hours=24))]['y'].values[0] if (x - pd.Timedelta(hours=24)) in df_prepared['ds'].values else np.nan)\n",
    "future['lag_168h'] = future['ds'].apply(lambda x: df_prepared[df_prepared['ds'] == (x - pd.Timedelta(hours=168))]['y'].values[0] if (x - pd.Timedelta(hours=168)) in df_prepared['ds'].values else np.nan)\n",
    "\n",
    "future['lag_24h'] = future['lag_24h'].fillna(mean_y)\n",
    "future['lag_168h'] = future['lag_168h'].fillna(mean_y)\n",
    "\n",
    "# Filter the future DataFrame to only include the *new* 30 days\n",
    "start_of_forecast = df_prepared['ds'].max() + pd.Timedelta(hours=1)\n",
    "future_forecast = future[future['ds'] >= start_of_forecast].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# 3. Generate the 30-Day Forecast\n",
    "forecast_30d = model.predict(future_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22e37a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define Risk: Risk increases with predicted impact (yhat) and with uncertainty (yhat_upper - yhat_lower)\n",
    "forecast_30d['Uncertainty_Band'] = forecast_30d['yhat_upper'] - forecast_30d['yhat_lower']\n",
    "\n",
    "# Calculate a Risk Score: High predicted volume + High uncertainty = High Risk\n",
    "# Risk Score = (Predicted Impact) + (Uncertainty Band)\n",
    "forecast_30d['Risk_Score'] = forecast_30d['yhat'] + forecast_30d['Uncertainty_Band']\n",
    "\n",
    "# 2. Extract key time components for analysis\n",
    "forecast_30d['Hour'] = forecast_30d['ds'].dt.hour\n",
    "forecast_30d['Date'] = forecast_30d['ds'].dt.strftime('%Y-%m-%d')\n",
    "forecast_30d['Day_of_Week'] = forecast_30d['ds'].dt.day_name()\n",
    "forecast_30d['Day_of_Month'] = forecast_30d['ds'].dt.day\n",
    "\n",
    "# 3. Find Global Minimum and Maximum Risk Hours\n",
    "min_risk_hour = forecast_30d.sort_values('Risk_Score', ascending=True).iloc[0]\n",
    "max_risk_hour = forecast_30d.sort_values('Risk_Score', ascending=False).iloc[0]\n",
    "\n",
    "# 4. Find Top 5 Deployment Windows (Lowest Risk)\n",
    "recommendations = forecast_30d.sort_values('Risk_Score', ascending=True).head(5)\n",
    "recommendations = recommendations[['ds', 'Day_of_Week', 'Hour', 'yhat', 'Risk_Score']].copy()\n",
    "recommendations.columns = ['Date_Time', 'Day_of_Week', 'Hour', 'Predicted_Impact (Norm)', 'Risk_Score']\n",
    "\n",
    "# 5. Analyze Risk by Hour of Day\n",
    "risk_by_hour = forecast_30d.groupby('Hour')['Risk_Score'].mean().sort_values(ascending=False).reset_index()\n",
    "\n",
    "# 6. Analyze Risk by Date of Month and Day of Week\n",
    "risk_by_month_day = forecast_30d.groupby('Day_of_Month')['Risk_Score'].mean().sort_values(ascending=False).reset_index()\n",
    "risk_by_day_of_week = forecast_30d.groupby('Day_of_Week')['Risk_Score'].mean().sort_values(ascending=False).reset_index()\n",
    "\n",
    "# 7. Analyze Risk by Hour and Day Combined\n",
    "risk_by_hour_day = forecast_30d.groupby(['Day_of_Week', 'Hour'])['Risk_Score'].mean().sort_values(ascending=False).reset_index()\n",
    "risk_by_hour_day['Rank'] = risk_by_hour_day['Risk_Score'].rank(method='dense', ascending=False).astype(int)\n",
    "\n",
    "\n",
    "print(\"\\n\\n--- 30-DAY DEPLOYMENT RISK ANALYSIS & RECOMMENDATIONS ---\")\n",
    "\n",
    "print(\"\\n[A] GLOBAL MINIMUM RISK DEPLOYMENT WINDOW (The Absolute Safest Hour)\")\n",
    "print(f\"Date/Time: {min_risk_hour['ds']}\")\n",
    "print(f\"Day of Week: {min_risk_hour['Day_of_Week']}\")\n",
    "print(f\"Predicted Impact (Norm): {min_risk_hour['yhat']:,.4f}\")\n",
    "print(f\"Minimum Risk Score: {min_risk_hour['Risk_Score']:,.4f}\")\n",
    "print(\"This is the hour with the lowest combination of predicted traffic and uncertainty in the entire month.\")\n",
    "\n",
    "\n",
    "print(\"\\n[B] GLOBAL MAXIMUM RISK DEPLOYMENT WINDOW (The Absolute Riskiest Hour)\")\n",
    "print(f\"Date/Time: {max_risk_hour['ds']}\")\n",
    "print(f\"Day of Week: {max_risk_hour['Day_of_Week']}\")\n",
    "print(f\"Predicted Impact (Norm): {max_risk_hour['yhat']:,.4f}\")\n",
    "print(f\"Maximum Risk Score: {max_risk_hour['Risk_Score']:,.4f}\")\n",
    "print(\"Deploying during this hour carries the highest combined traffic and uncertainty risk.\")\n",
    "\n",
    "\n",
    "print(\"\\n[C] TOP 5 LOW-RISK DEPLOYMENT WINDOWS (Next Best Suggestions)\")\n",
    "print(\"Based on the 60:40 weighted impact score (normalized 0 to 1).\")\n",
    "print(recommendations.to_markdown(index=False, floatfmt='.4f'))\n",
    "\n",
    "print(\"\\n[D] RISK ANALYSIS BY DATE OF MONTH (Where 1 is the riskiest day - Mean Risk)\")\n",
    "# Add rank for easy interpretation\n",
    "risk_by_month_day['Rank'] = risk_by_month_day['Risk_Score'].rank(method='dense', ascending=False).astype(int)\n",
    "print(risk_by_month_day[['Rank', 'Day_of_Month', 'Risk_Score']].head(5).to_markdown(index=False, floatfmt='.4f'))\n",
    "\n",
    "print(\"\\n[E] RISK ANALYSIS BY DAY OF WEEK (Where 1 is the riskiest day - Mean Risk)\")\n",
    "risk_by_day_of_week['Rank'] = risk_by_day_of_week['Risk_Score'].rank(method='dense', ascending=False).astype(int)\n",
    "# Reorder days for better readability\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "risk_by_day_of_week['Day_of_Week'] = pd.Categorical(risk_by_day_of_week['Day_of_Week'], categories=day_order, ordered=True)\n",
    "risk_by_day_of_week = risk_by_day_of_week.sort_values('Day_of_Week')\n",
    "print(risk_by_day_of_week[['Rank', 'Day_of_Week', 'Risk_Score']].to_markdown(index=False, floatfmt='.4f'))\n",
    "\n",
    "print(\"\\n[F] RISK ANALYSIS BY HOUR OF DAY (Where 1 is the riskiest hour - Mean Risk)\")\n",
    "risk_by_hour['Rank'] = risk_by_hour['Risk_Score'].rank(method='dense', ascending=False).astype(int)\n",
    "print(risk_by_hour[['Rank', 'Hour', 'Risk_Score']].to_markdown(index=False, floatfmt='.4f'))\n",
    "\n",
    "print(\"\\n[G] RISK ANALYSIS BY HOUR AND DAY COMBINED (Where 1 is the riskiest window - Mean Risk)\")\n",
    "print(\"This table ranks all 168 weekly hourly windows (Day + Hour) by their average risk score.\")\n",
    "risk_by_hour_day_safe = risk_by_hour_day.sort_values('Risk_Score', ascending=True).reset_index(drop=True)\n",
    "risk_by_hour_day_risky = risk_by_hour_day.sort_values('Risk_Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Reorder days for display\n",
    "day_order_display = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "risk_by_hour_day_risky['Day_of_Week'] = pd.Categorical(risk_by_hour_day_risky['Day_of_Week'], categories=day_order_display, ordered=True)\n",
    "risk_by_hour_day_safe['Day_of_Week'] = pd.Categorical(risk_by_hour_day_safe['Day_of_Week'], categories=day_order_display, ordered=True)\n",
    "\n",
    "\n",
    "print(\"\\nTop 10 Riskiest Windows:\")\n",
    "print(risk_by_hour_day_risky.head(10)[['Rank', 'Day_of_Week', 'Hour', 'Risk_Score']].to_markdown(index=False, floatfmt='.4f'))\n",
    "\n",
    "print(\"\\nTop 10 Safest Windows:\")\n",
    "print(risk_by_hour_day_safe.head(10)[['Rank', 'Day_of_Week', 'Hour', 'Risk_Score']].to_markdown(index=False, floatfmt='.4f'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
